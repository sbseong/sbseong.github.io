---
sort: 4
---

## Regression Study
### Linear regression


```
from sklearn.linear_model import LinearRegression
```


```
!pip install mglearn
```

    Collecting mglearn
    [?25l  Downloading https://files.pythonhosted.org/packages/65/38/8aced26fce0b2ae82c3c87cd3b6105f38ca6d9d51704ecc44aa54473e6b9/mglearn-0.1.9.tar.gz (540kB)
    [K     |β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–| 542kB 7.1MB/s 
    [?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mglearn) (1.18.5)
    Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from mglearn) (3.2.2)
    Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from mglearn) (0.22.2.post1)
    Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from mglearn) (1.0.5)
    Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from mglearn) (7.0.0)
    Requirement already satisfied: cycler in /usr/local/lib/python3.6/dist-packages (from mglearn) (0.10.0)
    Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from mglearn) (2.4.1)
    Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from mglearn) (0.16.0)
    Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mglearn) (2.4.7)
    Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mglearn) (1.2.0)
    Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mglearn) (2.8.1)
    Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->mglearn) (1.4.1)
    Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->mglearn) (2018.9)
    Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler->mglearn) (1.15.0)
    Building wheels for collected packages: mglearn
      Building wheel for mglearn (setup.py) ... [?25l[?25hdone
      Created wheel for mglearn: filename=mglearn-0.1.9-py2.py3-none-any.whl size=582639 sha256=8ca0814a4524c34a09786c1861ba4a99ad8a366bd0a054be5b0436e4d82235c6
      Stored in directory: /root/.cache/pip/wheels/eb/a6/ea/a6a3716233fa62fc561259b5cb1e28f79e9ff3592c0adac5f0
    Successfully built mglearn
    Installing collected packages: mglearn
    Successfully installed mglearn-0.1.9



```
import mglearn
from sklearn.model_selection import train_test_split
X, y = mglearn.datasets.make_wave(n_samples = 60)
```


```
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

lr = LinearRegression().fit(X_train, y_train)
```


```
print("lr.coef_: %.4f" %lr.coef_)
print("lr.intercept_: {}".format(lr.intercept_))
```

    lr.coef_: 0.3939
    lr.intercept_: -0.031804343026759746



```

```


```
print("ν›λ ¨ μ„ΈνΈ μ μ: {:.2f}".format(lr.score(X_train, y_train)))
print("ν…μ¤νΈ μ„ΈνΈ μ μ: {:.2f}".format(lr.score(X_test, y_test)))
```

    ν›λ ¨ μ„ΈνΈ μ μ: 0.67
    ν…μ¤νΈ μ„ΈνΈ μ μ: 0.66


μ„μ μƒν” μμ μ κ²½μ°λ” λ¨λΈμ΄ λ§¤μ° λ‹¨μν•μ—¬ overfitting(κ³Όλ€μ ν•©)λ¬Έμ λ¥Ό κ³ λ―Όν•  ν•„μ”λ” μ—†μµλ‹λ‹¤. 

Linear Regression λ¨λΈμ΄ λ³΄μ¤ν„΄ μ£Όνƒκ°€κ²© μ„Έμ΄ν„°μ…‹ κ°™μ€ λ³µμ΅ν• λ°μ΄ν„°μ…‹μ—μ„ μ–΄λ–»κ² λ™μ‘ν•λ” μ§€ ν•λ² μ‚΄ν΄λ³΄κ² μµλ‹λ‹¤.


```
X, y = mglearn.datasets.load_extended_boston()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

lr = LinearRegression().fit(X_train, y_train)
```


```
print("ν›λ ¨ μ„ΈνΈ μ μ: {:.2f}".format(lr.score(X_train, y_train)))
print("ν…μ¤νΈ μ„ΈνΈ μ μ: {:.2f}".format(lr.score(X_test, y_test)))
```

    ν›λ ¨ μ„ΈνΈ μ μ: 0.95
    ν…μ¤νΈ μ„ΈνΈ μ μ: 0.61


μ΄λ ‡κ² μƒν” 506κ°, νΉμ„±κ°’μ΄ 104κ°λ¥Ό μ“°κ²λλ©΄, κ³Όλ€μ ν•©λ¬Έμ κ°€ λ°μƒν•μ£ . ν›λ ¨κ³Ό ν…μ¤νΈ μ„ΈνΈμ μ μλ¥Ό λ³΄λ©΄ μ΄λ¥Ό μ μ¶”ν•  μ μμµλ‹λ‹¤.

### Ridge regression


λ¦Ώμ§€(Ridge) νκ·€λ„ μ„ ν•λ¨λΈμ„ μ‚¬μ©ν•λ―€λ΅ μµμ†μ ν•©λ²•(least square)μ—μ„ μ‚¬μ©ν• κ²ƒκ³Ό κ°™μ€ μμΈ΅ν•¨μλ¥Ό μ‚¬μ©ν•©λ‹λ‹¤. ν•μ§€λ§, λ¦Ώμ§€(Ridge) νκ·€μ—λ” μ μ•½μ΅°κ±΄μ„ λ‘ μκ°€ μμµλ‹λ‹¤. Wμ ν¬κΈ°λ¥Ό μ μ•½μΌλ΅ λ‘λ” regularization μ„ ν•λ” κ²ƒμ…λ‹λ‹¤.  λ¦Ώμ§€(Ridge) νκ·€μ—μ„λ” L2 regularizationμ„ μ‚¬μ©ν•©λ‹λ‹¤.


```
from sklearn.linear_model import Ridge
```


```
ridge = Ridge().fit(X_train, y_train)
print("ν›λ ¨ μ„ΈνΈ μ μ: {:.2f}".format(ridge.score(X_train, y_train)))
print("ν…μ¤νΈ μ„ΈνΈ μ μ: {:.2f}".format(ridge.score(X_test, y_test)))
```

    ν›λ ¨ μ„ΈνΈ μ μ: 0.89
    ν…μ¤νΈ μ„ΈνΈ μ μ: 0.75


κ²°κ³Όκ°€ Linear Regression(μ„ ν•νκ·€) λ¨λΈμ— λΉ„ν•΄μ„ ν…μ¤νΈ μ„ΈνΈ μ μκ°€ κ°μ„ λ κ²ƒμ„ λ³Ό μ μκ³ , ν•νΈ ν›λ ¨μ„ΈνΈμ κ²½μ° μ„±λ¥μ΄ λ‚®μ•„μ΅μµλ‹λ‹¤. μ„ ν•νκ·€μ κ²½μ°λ” κ³Όλ€μ ν•©μ΄ λμ—μ§€λ§, λ¦Ώμ§€νκ·€λ” λ μμ λ΅μ΄ λ¨λΈμ΄κΈ°μ— κ³Όλ€μ ν•©μ΄ μƒλ€μ μΌλ΅ μ μ–΄μ§‘λ‹λ‹¤. μ°λ¦¬μ μ΄μ μ€ ν…μ¤νΈ μ„ΈνΈμ΄κΈ° λ•λ¬Έμ— λ¦Ώμ§€ νκ·€λ¥Ό μ‚¬μ©ν•λ” κ²ƒμ΄ λ” λ°”λμ§ν•©λ‹λ‹¤.

**Q. κ·μ μ κ°•λ„λ” μ–΄λ–»κ² μ΅°μ ν• κΉμ”?** 

alpha κ°’μ„ μ΄μ©ν•μ—¬ μ΅°μ •ν•©λ‹λ‹¤. μ„ μμ μ—μ„λ” alpha = 1.0μ΄ μ‚¬μ©λμ—μΌλ‹, alphaκ°’μ„ λ†’μ΄λ©΄ κ³„μλ¥Ό 0μ— λ” κ°€κΉκ² λ§λ“¤μ–΄μ„ ν›λ ¨μ„±λ¥μ€ λ‚®μ•„μ§€κ³ , μΌλ°ν™”μ—λ” λ„μ›€μ„ μ¤λ‹λ‹¤.




```
ridge10 = Ridge(alpha=10).fit(X_train, y_train)
print("ν›λ ¨ μ„ΈνΈ μ μ: {:.2f}".format(ridge10.score(X_train, y_train)))
print("ν…μ¤νΈ μ„ΈνΈ μ μ: {:.2f}".format(ridge10.score(X_test, y_test)))
```

    ν›λ ¨ μ„ΈνΈ μ μ: 0.79
    ν…μ¤νΈ μ„ΈνΈ μ μ: 0.64



```
ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)
print("ν›λ ¨ μ„ΈνΈ μ μ: {:.2f}".format(ridge01.score(X_train, y_train)))
print("ν…μ¤νΈ μ„ΈνΈ μ μ: {:.2f}".format(ridge01.score(X_test, y_test)))
```

    ν›λ ¨ μ„ΈνΈ μ μ: 0.93
    ν…μ¤νΈ μ„ΈνΈ μ μ: 0.77


μ—¬κΈ°μ—μ„λ” alpha = 0.1μ΄ μΆ‹μ€ μ„±λ¥μ„ λ‚΄μ—μµλ‹λ‹¤. μ΄λ ‡κ² parameter(λ§¤κ°λ³€μ)λ¥Ό μ–΄λ–»κ² μ„¤μ •ν•λ” κ²ƒμ΄ μΆ‹μ€μ§€μ— λ€ν•΄μ„λ„ μ—°κµ¬κ°€ λ§μ΄ μμµλ‹λ‹¤. μ΄ λ‚΄μ©μ€ λ‹¤μμ— λ‹¤μ‹ λ‹¤λ¤„λ³΄κ² μµλ‹λ‹¤.


```
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

plt.plot(ridge10.coef_, '^', label="Ridge a10")
plt.plot(ridge.coef_, '^', label="Ridge a1")
plt.plot(ridge01.coef_, '^', label="Ridge a0.1")

plt.plot(lr.coef_, 'o', label="LinearRegression")
plt.xlabel("κ³„μ λ©λ΅")
plt.ylabel("κ³„μ ν¬κΈ°")
plt.hlines(0, 0, len(lr.coef_))
plt.ylim(-25, 25)
plt.legend()
plt.show()
```


![png](introduction_to_machine_learning_with_python_files/introduction_to_machine_learning_with_python_20_0.png)



```
mglearn.plots.plot_ridge_n_samples()
```


![png](introduction_to_machine_learning_with_python_files/introduction_to_machine_learning_with_python_21_0.png)


### Lasso

μ„ ν•νκ·€μ— κ·μ λ¥Ό μ μ©ν•λ”λ° Ridgeμ λ€ν•μΌλ΅ Lassoκ°€ μμµλ‹λ‹¤. κ·μ μ λ°©μ‹μ—μ„ μ°¨μ΄κ°€ μλ”λ°, L1 regularization(κ·μ )λ¥Ό μ‚¬μ©ν•©λ‹λ‹¤. μ΄ κ·μ μ νΉμ§•μ€ κ·μ μ κ²°κ³Όλ΅ νΉμ • κ³„μλ” 0μΌλ΅ λ§λ“¤κ² λ©λ‹λ‹¤. μ΄ λ§μ€ ν•΄λ‹Ή νΉμ„±μ€ μ™„μ „ν μ μ™Έν•κ³  λ³΄κ² λ‹¤λ” μλ―Έμ΄μ£ . νΉμ„±μ μ„ νƒκΉμ§€ μλ™μΌλ΅ μ΄λ¤„μ§„λ‹¤κ³  λ³΄λ©΄ λκ² μµλ‹λ‹¤. 


```
from sklearn.linear_model import Lasso
import numpy as np
```


```
lasso = Lasso().fit(X_train, y_train)
print("ν›λ ¨ μ„ΈνΈ μ μ: {:.2f}".format(lasso.score(X_train, y_train)))
print("ν…μ¤νΈ μ„ΈνΈ μ μ: {:.2f}".format(lasso.score(X_test, y_test)))
print("μ‚¬μ©ν• νΉμ„±μ μ: {}".format(np.sum(lasso.coef_ !=0)))
```

    ν›λ ¨ μ„ΈνΈ μ μ: 0.29
    ν…μ¤νΈ μ„ΈνΈ μ μ: 0.21
    μ‚¬μ©ν• νΉμ„±μ μ: 4


μ½”λ“μ—μ„ λ³Έ κ²ƒμ²λΌ, 104κ°μ νΉμ„± μ¤‘ λ‹¨ 4κ°λ§ μ‚¬μ©ν• κ²ƒμ„ λ³Ό μ μμµλ‹λ‹¤. Lassoμ—­μ‹ κ³„μλ¥Ό μ–Όλ§λ‚ κ°•ν•κ² 0μΌλ΅ λ³΄λ‚Ό μ§€ μ΅°μ ν•λ” alphaκ°’μ΄ μμΌλ©°, underfitting(κ³Όμ†μ ν•©)μ„ μ¤„μ΄κΈ° μ„ν•΄μ„ alphaκ°’μ„ μ¤„μ—¬λ³΄λ” κ²ƒμ΄ ν•„μ”ν•©λ‹λ‹¤.


```
lasso001 = lasso = Lasso(alpha = 0.01, max_iter=100000).fit(X_train, y_train)
```


```
print("ν›λ ¨ μ„ΈνΈ μ μ: {:.2f}".format(lasso001.score(X_train, y_train)))
print("ν…μ¤νΈ μ„ΈνΈ μ μ: {:.2f}".format(lasso001.score(X_test, y_test)))
print("μ‚¬μ©ν• νΉμ„±μ μ: {}".format(np.sum(lasso001.coef_ !=0)))
```

    ν›λ ¨ μ„ΈνΈ μ μ: 0.90
    ν…μ¤νΈ μ„ΈνΈ μ μ: 0.77
    μ‚¬μ©ν• νΉμ„±μ μ: 33



```
lasso00001 = lasso = Lasso(alpha = 0.0001, max_iter=100000).fit(X_train, y_train)

print("ν›λ ¨ μ„ΈνΈ μ μ: {:.2f}".format(lasso00001.score(X_train, y_train)))
print("ν…μ¤νΈ μ„ΈνΈ μ μ: {:.2f}".format(lasso00001.score(X_test, y_test)))
print("μ‚¬μ©ν• νΉμ„±μ μ: {}".format(np.sum(lasso00001.coef_ !=0)))
```

    ν›λ ¨ μ„ΈνΈ μ μ: 0.95
    ν…μ¤νΈ μ„ΈνΈ μ μ: 0.64
    μ‚¬μ©ν• νΉμ„±μ μ: 96



```
plt.plot(lasso.coef_, 's', label="Ridge a1")
plt.plot(lasso001.coef_, '^', label="Ridge a0.01")
plt.plot(lasso00001.coef_, 'v', label="Ridge a0.0001")

plt.plot(ridge01.coef_, 'o', label="Ridge a0.01")
plt.legend(ncol=2, loc=(0, 1.05))
```




    <matplotlib.legend.Legend at 0x7fba0b65bb38>




![png](introduction_to_machine_learning_with_python_files/introduction_to_machine_learning_with_python_30_1.png)


μ„ κ·Έλ¦Όμ—μ„ alphaμ ν¬κΈ°μ— λ”°λ¥Έ λ³€ν™”λ¥Ό λ³Ό μ μμµλ‹λ‹¤. alpha = 1μΌ λ•, κ³„μ λ€λ¶€λ¶„μ΄ 0μΌ λΏλ§ μ•„λ‹λΌ λ‚λ¨Έμ§€ κ³„μλ“¤λ„ ν¬κΈ°κ°€ μ‘λ‹¤λ” κ²ƒμ„ λ³Ό μ μμµλ‹λ‹¤. alphaκ°€ μ‘μ•„μ§€λ©΄, κ·μ λ¥Ό λ” μ κ² λ°›λ” κ²ƒμ΄κ³ , 0.00001μ alphaκ°’μ„ μ£Όλ©΄ κ±°μ κ·μ λ¥Ό λ°›μ§€ μ•λ” κ²ƒμ„ ν™•μΈν•  μ μμ—μµλ‹λ‹¤. alpha=0.01μΈ Ridgeμ™€ lassoλ” μ„±λ¥μ΄ λΉ„μ·ν•μ§€λ§, Ridgeμ κ²½μ°λ” μ–΄λ–¤ κ³„μλ„ 0μ΄ λμ§€ μ•λ” λ‹¤λ” μ !

λ‘ κ·μ λ°©μ‹μ ν¨λ„ν‹°λ¥Ό κ²°ν•©ν• ν•νƒμ ElasticNetλ„ μμΌλ‹, μ΄ κ²ƒμ„ μ΄μ©ν•λ©΄ μµμƒμ μ„±λ¥μ„ λ‚Ό μ μμ„ κ²ƒμ΄κ³ , ν•νΈ 2κ°€μ§€μ parameterλ¥Ό μ΅°μ ν•΄μ£Όμ–΄μ•Ό ν•©λ‹λ‹¤. L1, L2 κ°κ°μ alphaκ°’μ„μ”!


```

```


```

```


```

```


```

```


```

```
